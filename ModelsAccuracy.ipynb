{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAeHNdWEuOBW",
        "outputId": "4bb2d535-0331-4998-fb61-40c87ba76b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Last Alive Less Initial Pathologic Diagnosis Date Calculated Day Value'\n",
            " 'Neoplasm Histologic Grade' 'Primary Lymph Node Presentation Assessment'\n",
            " 'Number of Samples Per Patient' 'Patient Weight']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance Comparison (Accuracy Scores):\n",
            "Random Forest: 0.9051\n",
            "XGBoost: 0.9430\n",
            "SVM: 0.8671\n",
            "Decision Tree: 0.8671\n",
            "Neural Network: 0.8734\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# ---- STEP 1: Load Dataset ----\n",
        "file_path = \"/NSCLC ML RESEARCH.csv\"  # Update with correct file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Drop irrelevant columns\n",
        "irrelevant_columns = [\"Study ID\", \"Patient ID\", \"Sample ID\", \"Form completion date\", \"Other Patient ID\"]\n",
        "df_cleaned = df.drop(columns=irrelevant_columns, errors='ignore')\n",
        "\n",
        "# Handle missing values: Fill numerical with mean, categorical with mode\n",
        "for col in df_cleaned.columns:\n",
        "    if df_cleaned[col].dtype == \"object\":\n",
        "        df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mode()[0])  # Fill categorical with mode\n",
        "    else:\n",
        "        df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mean())  # Fill numerical with mean\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "for col in df_cleaned.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    df_cleaned[col] = le.fit_transform(df_cleaned[col])\n",
        "    label_encoders[col] = le  # Store label encoders for inverse transformation if needed\n",
        "\n",
        "# Select target variable (Overall Survival Status)\n",
        "target_column = \"Overall Survival Status\"\n",
        "X = df_cleaned.drop(columns=[target_column])  # Features\n",
        "y = df_cleaned[target_column]  # Target\n",
        "\n",
        "# Handle remaining missing values using Imputer\n",
        "imputer = SimpleImputer(strategy=\"mean\")  # Replace NaNs with column mean\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "# Split data into Training (70%), Validation (15%), and Test (15%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.30, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42)\n",
        "\n",
        "# ---- STEP 2: Train & Evaluate Models ----\n",
        "model_results = {}\n",
        "\n",
        "# ---- Train Random Forest ----\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "model_results[\"Random Forest\"] = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "# ---- Train XGBoost ----\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.05, eval_metric='logloss')\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "model_results[\"XGBoost\"] = accuracy_score(y_test, y_pred_xgb)\n",
        "\n",
        "# ---- Train SVM ----\n",
        "svm_model = SVC(kernel='rbf', probability=True)\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "model_results[\"SVM\"] = accuracy_score(y_test, y_pred_svm)\n",
        "\n",
        "# ---- Train Decision Tree ----\n",
        "dt_model = DecisionTreeClassifier(max_depth=4)\n",
        "dt_model.fit(X_train, y_train)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "model_results[\"Decision Tree\"] = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# ---- Train Neural Network ----\n",
        "nn_model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification (Survival: Yes/No)\n",
        "])\n",
        "\n",
        "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "nn_model.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_val, y_val), verbose=0)\n",
        "test_loss, test_acc_nn = nn_model.evaluate(X_test, y_test, verbose=0)\n",
        "model_results[\"Neural Network\"] = test_acc_nn\n",
        "\n",
        "# ---- STEP 3: Compare Model Performances ----\n",
        "print(\"\\nModel Performance Comparison (Accuracy Scores):\")\n",
        "for model, acc in model_results.items():\n",
        "    print(f\"{model}: {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2jDF79QKuRnR"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}